# tiny_engine

A minimal implementation of the [GameN-Gen paper](https://arxiv.org/pdf/2408.14837), a V-World Model for Video Game Environment Generation.

This project trains a diffusion model to generate video game footage conditioned on player actions. It uses a pre-trained VAE from Stable Diffusion to work in the latent space and trains a UNet on video data from the ViZDoom environment.

## Model Architecture

The model, `GameNGen`, is a latent video diffusion model.
-   **Autoencoder (VAE)**: It uses a pretrained VAE from `CompVis/stable-diffusion-v1-4` to encode video frames into a low-dimensional latent space. The VAE is frozen during training.
-   **UNet**: A `UNet2DConditionModel`, also from Stable Diffusion, is trained to denoise latents.
-   **Conditioning**: The UNet is conditioned on:
    1.  The latent representation of the previous frame.
    2.  The player's action, which is passed through a small feed-forward network and combined with the frame embedding.
-   **Scheduler**: A `DDPMScheduler` is used to manage the diffusion process (adding noise during training and denoising during inference).

## Getting Started

### Prerequisites

You can install the required packages using pip. It is recommended to use a virtual environment.

```bash
pip install torch diffusers torchvision pandas pillow stable-baselines3[extra] transformers accelerate tqdm vizdoom huggingface-hub opencv-python
```

*Note: `vizdoom` may have additional system dependencies. Please refer to the official [ViZDoom installation guide](https://github.com/Farama-Foundation/ViZDoom) for more details.*

### Training

The training script `train.py` will automatically download the dataset from [RevanthGundala/vizdoom](https://huggingface.co/datasets/RevanthGundala/vizdoom) on the Hugging Face Hub.

To start training, simply run:

```bash
python train.py
```

Model checkpoints (the UNet and the action encoder) will be saved to the `output/` directory by default.

## Dataset Generation (Optional)

The training data used in this project was generated by running a PPO agent from `stable-baselines3` in the ViZDoom environment. The script `agent.py` can be used to generate a similar dataset.

When you run `agent.py`, it will:
1.  Initialize a ViZDoom environment.
2.  Run a PPO agent to play the game.
3.  Save screen frames and corresponding actions to the `gamelogs/` directory.

You would then need to process this data into the video format and `metadata.csv` file that `train.py` expects.

## Future Work

-   An inference script to generate videos with a trained model.
-   Support for other environments.
-   More complex agent actions.
